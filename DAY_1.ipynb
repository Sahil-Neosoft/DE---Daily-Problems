{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTMqJ8_jcCTP",
        "outputId": "058c1222-3301-4533-b500-3d84241d9826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "f7pzDwixddNE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: PySpark – Detect Missing Dates in a Time Series\n"
      ],
      "metadata": {
        "id": "8W-EwYOP2RqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have daily sales data, but some dates are missing. Find all missing dates in the range."
      ],
      "metadata": {
        "id": "GEA02TWm2S6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"2025-01-01\", 100),\n",
        "        (\"2025-01-02\", 150),\n",
        "        (\"2025-01-04\", 120),\n",
        "        (\"2025-01-06\", 200)]\n",
        "columns = [\"date\", \"sales\"]\n",
        "\n",
        "sales = spark.createDataFrame(data, columns)\n",
        "sales = sales.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))"
      ],
      "metadata": {
        "id": "tywmljKS2QzE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "# Min and Max date\n",
        "date_min=sales.select(min(\"date\")).collect()[0][0]\n",
        "date_max=sales.select(max(\"date\")).collect()[0][0]\n",
        "\n",
        "# Date exploding\n",
        "full_dates=spark.createDataFrame([(date_min,date_max)],[\"start\",\"end\"])\\\n",
        "          .select(explode(sequence(col(\"start\"),col(\"end\"))).alias(\"date\"))\n",
        "\n",
        "# Missing dates\n",
        "missing_dates=full_dates.join(sales,on=\"date\",how=\"left_anti\")\n",
        "\n",
        "missing_dates.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HYPF6pi2pbR",
        "outputId": "33c9d416-7d37-48a3-da58-c85ec6f9a313"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|      date|\n",
            "+----------+\n",
            "|2025-01-05|\n",
            "|2025-01-03|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: SQL – Find the Longest Streak of Active Days per User"
      ],
      "metadata": {
        "id": "-bBXwlh39Ud4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a SQL table user_logins(user_id, login_date). Write a SQL query to find the longest streak of consecutive login days for each user."
      ],
      "metadata": {
        "id": "ehzNH6XH9fBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"u1\", \"2025-08-01\"),\n",
        "    (\"u1\", \"2025-08-02\"),\n",
        "    (\"u1\", \"2025-08-03\"),\n",
        "    (\"u1\", \"2025-08-05\"),\n",
        "    (\"u2\", \"2025-08-01\"),\n",
        "    (\"u2\", \"2025-08-03\"),\n",
        "    (\"u2\", \"2025-08-04\"),\n",
        "    (\"u3\", \"2025-08-01\"),\n",
        "    (\"u3\", \"2025-08-02\"),\n",
        "    (\"u3\", \"2025-08-04\"),\n",
        "    (\"u3\", \"2025-08-05\"),\n",
        "    (\"u3\", \"2025-08-06\"),\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"login_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "user_logins_df = spark.createDataFrame(data, schema) \\\n",
        "    .withColumn(\"login_date\", to_date(\"login_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "user_logins_df.createOrReplaceTempView(\"user_logins\")"
      ],
      "metadata": {
        "id": "U6JDYwOmFPa8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=spark.sql(\n",
        "    \"\"\"\n",
        "    with ordered as(\n",
        "      select user_id,login_date,\n",
        "      row_number() over(partition by user_id order by login_date) as rn\n",
        "      from user_logins\n",
        "    ),\n",
        "    grouped as (\n",
        "      select user_id, login_date,\n",
        "      date_sub(login_date,rn) as grp\n",
        "      from ordered\n",
        "    ),\n",
        "    streak as (\n",
        "      select user_id, count(*) as streak_length from grouped\n",
        "      group by user_id,grp\n",
        "    )\n",
        "\n",
        "    select user_id,max(streak_length)as longest_streak from streak\n",
        "    group by user_id order by user_id;\n",
        "    \"\"\"\n",
        ")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRwgM7Xx9NU9",
        "outputId": "fabd2336-b385-4ade-84b3-48eb15bcc05b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "|user_id|longest_streak|\n",
            "+-------+--------------+\n",
            "|     u1|             3|\n",
            "|     u2|             2|\n",
            "|     u3|             3|\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: PySpark – Find the Top-Selling Product per Day\n",
        "\n"
      ],
      "metadata": {
        "id": "SR9pMFdycK1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a PySpark DataFrame sales containing daily product sales. Write a PySpark program to find the top-selling product for each day based on total sales amount.\n",
        "\n"
      ],
      "metadata": {
        "id": "-muMXyKGcvVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"2025-02-01\", \"A\", 100),\n",
        "    (\"2025-02-01\", \"B\", 200),\n",
        "    (\"2025-02-01\", \"C\", 150),\n",
        "    (\"2025-02-02\", \"A\", 180),\n",
        "    (\"2025-02-02\", \"B\", 120),\n",
        "    (\"2025-02-02\", \"C\", 220),\n",
        "]\n",
        "columns = [\"date\", \"product\", \"sales_amount\"]\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sales = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "leQB_0Y6cgsg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Aggregate sales\n",
        "  daily_sales = sales.groupBy(\"date\",\"product\").agg(sum(col(\"sales_amount\")).alias(\"sales_amount\"))\n",
        "\n",
        "  # # Window partition by date\n",
        "  window= Window.partitionBy(\"date\").orderBy(col(\"sales_amount\").desc())\n",
        "\n",
        "  # # Top-selling product\n",
        "  top_selling_product= daily_sales.withColumn(\"rank\",rank().over(window)).filter(col(\"rank\")==1).select(\"date\",\"product\",\"sales_amount\")\n",
        "  top_selling_product.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nir3sHnGd4ak",
        "outputId": "91f8e3d8-6244-41e3-e9d4-3ff3ee95fbb7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+------------+\n",
            "|      date|product|sales_amount|\n",
            "+----------+-------+------------+\n",
            "|2025-02-01|      B|         200|\n",
            "|2025-02-02|      C|         220|\n",
            "+----------+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4: SQL – Find Users Who Purchase for N Consecutive Days"
      ],
      "metadata": {
        "id": "LKgHBcm7lCAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a table purchases(user_id, purchase_date), write a SQL query to find all users who purchased items for at least 3 consecutive days."
      ],
      "metadata": {
        "id": "_uXKxWFklEiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"u1\", \"2025-08-01\"),\n",
        "    (\"u1\", \"2025-08-02\"),\n",
        "    (\"u1\", \"2025-08-03\"),\n",
        "    (\"u1\", \"2025-08-05\"),\n",
        "    (\"u2\", \"2025-08-01\"),\n",
        "    (\"u2\", \"2025-08-04\"),\n",
        "    (\"u2\", \"2025-08-05\"),\n",
        "    (\"u2\", \"2025-08-06\"),\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"purchase_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "purchases_df = spark.createDataFrame(data, schema) \\\n",
        "    .withColumn(\"purchase_date\", to_date(\"purchase_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "purchases_df.createOrReplaceTempView(\"purchases\")"
      ],
      "metadata": {
        "id": "6ba6R3iGF6_X"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"\"\"\n",
        "    with ordered as (\n",
        "      select user_id,purchase_date,\n",
        "      row_number() over(partition by user_id order by purchase_date) as rn\n",
        "      from purchases\n",
        "),\n",
        "grouped as (\n",
        "      select user_id,purchase_date,\n",
        "      date_sub(purchase_date,rn) as grp\n",
        "      from ordered\n",
        "),\n",
        "streaks as (\n",
        "      select user_id,count(*) as streak_length from grouped\n",
        "      group by user_id,grp\n",
        ")\n",
        "\n",
        "select distinct user_id\n",
        "from streaks\n",
        "where streak_length >=3\n",
        "\"\"\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_rQiA4zeSyF",
        "outputId": "030b761f-7d85-4b92-881c-a5cdbeee612d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|user_id|\n",
            "+-------+\n",
            "|     u1|\n",
            "|     u2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}