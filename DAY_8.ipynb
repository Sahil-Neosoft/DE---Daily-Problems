{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQhsAy-I3wSv",
        "outputId": "862ec6ac-68d0-49fb-eac6-80e5417f1e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "spark=SparkSession.builder.appName(\"Day 8\").getOrCreate()"
      ],
      "metadata": {
        "id": "G3DyTl7033B5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: PySpark – Calculate Rolling 3-Day Average of Sales"
      ],
      "metadata": {
        "id": "FY2t8pkA4jOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have a PySpark DataFrame containing daily sales data. Write a PySpark program to calculate the rolling 3-day average sales for each date, ordered by the date column.\n",
        "\n"
      ],
      "metadata": {
        "id": "vPm_J1BQ4okj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"2025-01-01\", 100),\n",
        "    (\"2025-01-02\", 200),\n",
        "    (\"2025-01-03\", 300),\n",
        "    (\"2025-01-04\", 400),\n",
        "    (\"2025-01-05\", 500),\n",
        "]\n",
        "\n",
        "columns = [\"sale_date\", \"sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "0ULf9ocE4WTF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7shlOrwB_rj",
        "outputId": "da5cf224-3913-4499-ba44-7ae8dbef4f44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "| sale_date|sales|\n",
            "+----------+-----+\n",
            "|2025-01-01|  100|\n",
            "|2025-01-02|  200|\n",
            "|2025-01-03|  300|\n",
            "|2025-01-04|  400|\n",
            "|2025-01-05|  500|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " window=Window.orderBy(\"sale_date\").rowsBetween(-2,0)\n",
        " result=df.withColumn(\"rolling_3_day_avg\",avg(\"sales\").over(window)).select(\"sale_date\",\"sales\",\"rolling_3_day_avg\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5TvAjGF5IoY",
        "outputId": "5fd1cca5-04a0-4c97-9de2-1ba585039a8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------------+\n",
            "| sale_date|sales|rolling_3_day_avg|\n",
            "+----------+-----+-----------------+\n",
            "|2025-01-01|  100|            100.0|\n",
            "|2025-01-02|  200|            150.0|\n",
            "|2025-01-03|  300|            200.0|\n",
            "|2025-01-04|  400|            300.0|\n",
            "|2025-01-05|  500|            400.0|\n",
            "+----------+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: SQL – Find Customers with Increasing Purchase Amounts"
      ],
      "metadata": {
        "id": "337ZW_8aCgcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have a SQL table purchases(customer_id, purchase_date, amount). Write a query to find customers whose purchase amounts strictly increased with each new purchase date."
      ],
      "metadata": {
        "id": "aDB-mIe2CjZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"C1\", \"2025-01-01\", 100),\n",
        "    (\"C1\", \"2025-01-05\", 200),\n",
        "    (\"C1\", \"2025-01-10\", 300),\n",
        "    (\"C2\", \"2025-01-02\", 150),\n",
        "    (\"C2\", \"2025-01-06\", 120),\n",
        "    (\"C3\", \"2025-01-03\", 200),\n",
        "    (\"C3\", \"2025-01-09\", 250),\n",
        "]\n",
        "\n",
        "columns = [\"customer_id\", \"purchase_date\", \"amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df = df.withColumn(\"purchase_date\", to_date(\"purchase_date\"))\n",
        "\n",
        "df.createOrReplaceTempView(\"purchases\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbZ3a3u-Cg6G",
        "outputId": "ce1bb231-1e13-48a6-a3a3-1136966de1dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------+\n",
            "|customer_id|purchase_date|amount|\n",
            "+-----------+-------------+------+\n",
            "|         C1|   2025-01-01|   100|\n",
            "|         C1|   2025-01-05|   200|\n",
            "|         C1|   2025-01-10|   300|\n",
            "|         C2|   2025-01-02|   150|\n",
            "|         C2|   2025-01-06|   120|\n",
            "|         C3|   2025-01-03|   200|\n",
            "|         C3|   2025-01-09|   250|\n",
            "+-----------+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    select customer_id\n",
        "    from\n",
        "    (\n",
        "    select customer_id,amount,\n",
        "    lag(amount) over(partition by customer_id order by purchase_date) prev_amount\n",
        "    from purchases\n",
        "    )\n",
        "    group by customer_id\n",
        "    having sum(\n",
        "      case\n",
        "          when amount<=prev_amount then 1\n",
        "          else 0\n",
        "      end\n",
        "    )=0;\n",
        "    \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfsiyrwbD5UH",
        "outputId": "7fe6072c-bf6c-46ee-d9cb-41b8cff40490"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|         C1|\n",
            "|         C3|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}